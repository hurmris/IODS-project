# 4. Clustering and classification

```{r, echo=FALSE, warning=FALSE}
library(MASS)
library(plotly)
library(reshape2)
```

## Data

### Read data

```{r}
data(Boston)
str(Boston)
```

### Variable descriptions

Boston data set provides Housing Values in Suburbs of Boston. The detailed description of variables is given below.

Variables:

-   crim: Per capita crime rate by town.
-   zn: Proportion of residential land zoned for lots over 25,000 sq.ft.
-   indus: Proportion of non-retail business acres per town.
-   chas: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).
-   nox: Nitrogen oxides concentration (parts per 10 million).
-   rm: Average number of rooms per dwelling.
-   age: Proportion of owner-occupied units built prior to 1940.
-   dis: Weighted mean of distances to five Boston employment centres.
-   rad: Index of accessibility to radial highways.
-   tax: Full-value property-tax rate per 10,000 dollars.
-   ptratio: Pupil-teacher ratio by town.
-   black: Proportion of the population that identifies as Black in each town.
-   lstat: Lower status of the population (percent).
-   medv: Median value of owner-occupied homes in 1000s (dollars).

### Data overview

#### Distributions

```{r}

melted_data <- melt(Boston)

# Plot the distributions using ggplot2
ggplot(melted_data, aes(x = value)) +
  geom_histogram(binwidth = 1, fill = "pink", color = "black") +
  facet_wrap(~variable, scales = "free") +
  theme_minimal()+
  labs(title="Distirbution of variables in Boston data set")

```

The above plot describes the distribution of all the variables in the Boston data set. Few key observations can be made:

-   Distributions for crim, zn, age and black are highly skewed
-   Nitrogen oxides concentration in most suburds is below 0.5
-   Dwellings are quite large, most of them havin 6 to 7 rooms on average.
-   tax show that the sample includes suburbs with very different full-value property-tax rangin from 200 to nearly 700.

### Correlation plot

```{r}
# calculate the correlation matrix and round it
cor_matrix <- round(cor(Boston),2)

# visualize the correlation matrix
library(corrplot)
corrplot(cor_matrix, method="circle",tl.cex = 0.6, title = "Correlation plot")

```

From the above correlation plot we can observe that there are many varibles with high correlations. The pairs with high negative correlation include e.g medv-lstat, age-dis,dis-indus. Paris with high positive correalation include e.g medvd-rm, nox-age, zn-dis. All these are rather expected based on the earlier findings in urban research. 

## Linear discriminant analysis
In this anaysis I fit linear discriminant analysis (LDA) on the Boston data set. LDA is fitted to train data set and the prediction performance is then evaluated in the tests data. First start by standardizing the data set. 

### Standardizing data

```{r}
# Scale the Boston data set and save to boston_scaled
boston_scaled <- scale(Boston) %>% as.data.frame()

summary(boston_scaled)
round(var(boston_scaled),2)
```
From the above summaries we see that the scale function centers each variables so that they have mean zero and variance of 1. 

### Test and train data sets
Then create create a categorical variable of the crime rate in the Boston dataset and split data into test and train sets: 

```{r}

# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)

# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins,labels = c("low","med_low","med_high","high"), include.lowest = TRUE)

# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)

# Test and train data sets:
n <- nrow(boston_scaled)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]

```

### Fitting the model

```{r}
# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  graphics::arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda (bi)plot
plot(lda.fit, dimen = 2)
lda.arrows(lda.fit, myscale = 1)

```

### Model performance

```{r}
# save the correct classes from test data
correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)

# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)

```
The above cross tabulation suggests that the model does pretty well job predicting the high criminality cases: all of the 24 high criminality suburbs are predicted to be in the high criminality category. For the other categories the model doesn't perform that well as we see based on the dispersion.  

## k-means clustering -approach

### Standardizing the data
Start by scaling the variables to get comparable distances.
```{r}
data("Boston")
boston_scaled <- scale(Boston) %>% as.data.frame()
```

### Distances
Calculate then the euclidean distances between the observations:

```{r}
dist_eu <- dist(boston_scaled)
summary(dist_eu)
```


### k-means algorithm

```{r}
set.seed(123)

# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(Boston, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')

```
It seems that the the total of within cluster sum of squares (WCSS) drops at 2. Hence this is likely to be optimal number of clusters:

```{r}
# k-means clustering
km <- kmeans(Boston, centers = 2)

# plot the Boston dataset with clusters
pairs(Boston, col = km$cluster)

```
In the figure above is matrix of scatter plots where the colors (black and red) correspond to the two clusters produced by k-means clustering procedure. Two variables however stands out. These are rad and tax. It seems that these are only two variables that simple guessing analysis could correctly classify each observation to a cluster. In other cells of the matrix we see that the red and black are scattered with out any clear pattern.


## Bonus

```{r}
#Data
set.seed(13)
data("Boston")
boston_scaled <- scale(Boston) %>% as.data.frame()

#Set three clusters:
km_bonus <- kmeans(Boston, centers = 4)
boston_scaled$cluster <- km_bonus$cluster

bonus_fit <- lda(cluster ~ ., data = boston_scaled)

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  graphics::arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(boston_scaled$cluster)

# plot the lda (bi)plot
plot(bonus_fit, dimen =2)
lda.arrows(bonus_fit, myscale = 1)

bonus_fit


```
It seems that the most influential variables are black, rad and tax. 


## Super-Bonus
```{r}
model_predictors <- dplyr::select(train, -crime)
# check the dimensions
dim(model_predictors)
dim(lda.fit$scaling)
# matrix multiplication
matrix_product <- as.matrix(model_predictors) %*% lda.fit$scaling
matrix_product <- as.data.frame(matrix_product)

plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers',color = train$crime)

```



